{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating lwlrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition uses a metric called lwlrap. It's described as:\n",
    "\n",
    "> lwlrap measures the average precision of retrieving a ranked list of relevant labels for each test clip (i.e., the system ranks all the available labels, then the precisions of the ranked lists down to the true label are averaged). This is a generalization of the mean reciprocal rank measure (used in last year's edition of the competition) for the case where there can be multiple true labels per test item. \n",
    "\n",
    ">The novel \"label-weighted\" part means that the overall score is the average over all the labels in the test set, where each label receives equal weight (by contrast, plain lrap gives each test item equal weight, thereby discounting the contribution of individual labels when they appear on the same item as multiple other labels).\n",
    "\n",
    "> We use label weighting because it allows per-class values to be calculated, and still have the overall metric be expressed as a simple average of the per-class metrics (weighted by each label's prior in the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the starter code from the competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
    "  \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
    "  # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
    "  sample_weight = np.sum(truth > 0, axis=1)\n",
    "  nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "  overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n",
    "      truth[nonzero_weight_sample_indices, :] > 0, \n",
    "      scores[nonzero_weight_sample_indices, :], \n",
    "      sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "  return overall_lwlrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the sklearn and full versions of `lwlrap` are beyond my understanding. But we can still build intuition about how this metric **feels**. What makes it go up? What makes it go down?\n",
    "\n",
    "Let's create 100 samples on data with 10 labels and see what a \"random\" lwlrap looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.6463328664799254\n",
      "lwlrap from sklearn.metrics = 0.6320509097948122\n",
      "lwlrap from sklearn.metrics = 0.6751874304457333\n",
      "lwlrap from sklearn.metrics = 0.6460547504025762\n",
      "lwlrap from sklearn.metrics = 0.6078892474169683\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # Random test data.\n",
    "    num_samples = 100\n",
    "    num_labels = 10\n",
    "    truth = np.random.rand(num_samples, num_labels) > 0.5\n",
    "    scores = np.random.rand(num_samples, num_labels)\n",
    "    print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for 100 samples with 10 random labels, we see values around `0.6`-`0.7`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of labels to `80` since that's what our competition uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.5361859180299589\n",
      "lwlrap from sklearn.metrics = 0.5328551826559567\n",
      "lwlrap from sklearn.metrics = 0.5481219726196813\n",
      "lwlrap from sklearn.metrics = 0.5186607084213624\n",
      "lwlrap from sklearn.metrics = 0.5364357582444282\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # Random test data.\n",
    "    num_samples = 100\n",
    "    num_labels = 80\n",
    "    truth = np.random.rand(num_samples, num_labels) > 0.5\n",
    "    scores = np.random.rand(num_samples, num_labels)\n",
    "    print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see values closer to `0.5`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of samples to `1,120` because that's how many test items there are in our competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.5298257037088475\n",
      "lwlrap from sklearn.metrics = 0.5313353766293242\n",
      "lwlrap from sklearn.metrics = 0.5284328382110621\n",
      "lwlrap from sklearn.metrics = 0.5336294318861579\n",
      "lwlrap from sklearn.metrics = 0.5315035852546283\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # Random test data.\n",
    "    num_samples = 1120\n",
    "    num_labels = 80\n",
    "    truth = np.random.rand(num_samples, num_labels) > 0.5\n",
    "    scores = np.random.rand(num_samples, num_labels)\n",
    "    print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's no apparent change based solely on the number of test items.\n",
    "\n",
    "Right now we're assuming about half the labels are `True` and half are `False`. In our competition usually only 1 or 2 labels are `True`. Let's try to account for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.08389730170935047\n",
      "lwlrap from sklearn.metrics = 0.0857571538950253\n",
      "lwlrap from sklearn.metrics = 0.08043748764821373\n",
      "lwlrap from sklearn.metrics = 0.08462396214727662\n",
      "lwlrap from sklearn.metrics = 0.08541145280375675\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    # Random test data.\n",
    "    num_samples = 1120\n",
    "    num_labels = 80\n",
    "    truth = np.random.rand(num_samples, num_labels) < (2/80)\n",
    "    scores = np.random.rand(num_samples, num_labels)\n",
    "    print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. That's a large drop. So if we have completely random scores with sparsely `True` labels, we should expect scores around `0.08`. Our lowest submitted scores were about `0.06` so this seems correct. Any difference is probably due to my `(2/80)` approximation which doesn't guarantee that a label is always present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we influence lwlrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some intuition about what makes it go up and down, can we increase lwlwrap? In order to make this easier, we'll just look at a single example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.19642857142857142\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1\n",
    "num_labels = 10\n",
    "#This time we'll just set the first two labels True to make things easier to look at\n",
    "truth = np.zeros((1,num_labels), dtype=bool)\n",
    "truth[0][0] = True\n",
    "truth[0][1] = True\n",
    "\n",
    "scores = np.random.rand(1, num_labels)\n",
    "\n",
    "print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, False, False, False, False, False, False,\n",
       "        False]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31620831, 0.73221285, 0.73619461, 0.93265508, 0.96794554,\n",
       "        0.12174799, 0.14749704, 0.98200028, 0.94924133, 0.98628111]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12174798641008422, 0.9862811094135545)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(scores), np.max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, 5, 7, 0, 1, 8, 6, 9])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = scores[0].argsort()\n",
    "ranks = np.empty_like(temp)\n",
    "ranks[temp] = np.arange(len(scores[0]))\n",
    "ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a set of predictions and a given ranking of those predictions. Let's keep the same ranking but make the distance between successive predictions much smaller (0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12374799, 0.12474799, 0.12574799, 0.12674799, 0.12874799,\n",
       "        0.12174799, 0.12274799, 0.12974799, 0.12774799, 0.13074799]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sameButDiff = np.zeros_like(scores)\n",
    "for pos, val in enumerate(ranks):    \n",
    "    sameButDiff[0][pos] = np.min(scores) + (0.001 * val)\n",
    "    \n",
    "sameButDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.19642857142857142\n"
     ]
    }
   ],
   "source": [
    "print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, sameButDiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like as long as we keep the order the same, the distance between predictions has no influence over the score.\n",
    "\n",
    "What happens if we shift the scores to be around 0.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.502, 0.503, 0.504, 0.505, 0.507, 0.5  , 0.501, 0.508, 0.506,\n",
       "        0.509]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sameButDiff = np.zeros_like(scores)\n",
    "for pos, val in enumerate(ranks):    \n",
    "    sameButDiff[0][pos] = 0.5 + (0.001 * val)\n",
    "    \n",
    "sameButDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwlrap from sklearn.metrics = 0.19642857142857142\n"
     ]
    }
   ],
   "source": [
    "print(\"lwlrap from sklearn.metrics =\", calculate_overall_lwlrap_sklearn(truth, sameButDiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the position of the scores doesn't matter either. This means that it's **good** to have an unconfident model that gets the relative order correct. It's completely fine if all our predictions are around 0.5.\n",
    "\n",
    "This is useful knowledge for us, but probably bad for the competition owners. If they want to build a model for out of sample data, having a model that predi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
