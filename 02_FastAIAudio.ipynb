{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAI Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current solution uses images that are saved to disk but we can build one that uses them from memory instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.52.dev0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import fastai\n",
    "#from fastai.imports import *\n",
    "from fastai.basics import DataBunch, ItemList,DatasetType,ItemBase,Optional,Collection,Callable,Tensor,Iterator,PathOrStr\n",
    "from fastai.metrics import accuracy, accuracy_thresh, fbeta\n",
    "#from fastai.torch_core import *\n",
    "\n",
    "from fastai.callbacks.hooks import num_features_model, hook_output\n",
    "from fastai.vision import create_body, create_head, Image, channel_view, normalize_funcs, models\n",
    "from fastai.vision.learner import cnn_config, _resnet_split, ClassificationInterpretation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import IPython\n",
    "import IPython.display\n",
    "import PIL\n",
    "import time\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import display, Audio\n",
    "import importlib\n",
    "#Josh TODO: What is this find_spec thing?\n",
    "soundfile_spec = importlib.util.find_spec(\"soundfile\")\n",
    "if soundfile_spec is not None:\n",
    "    import soundfile as sf\n",
    "\n",
    "import librosa \n",
    "import torch\n",
    "import gc\n",
    "\n",
    "start_time = time.time()\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClip(ItemBase):\n",
    "    def __init__(self, signal, sample_rate, fn):\n",
    "        self.data = signal # Contains original signal to start \n",
    "        self.original_signal = signal.clone()\n",
    "        self.processed_signal = signal.clone()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.fn = fn\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(duration={}s, sample_rate={:.1f}KHz)'.format(\n",
    "            self.duration, self.sample_rate/1000)\n",
    "\n",
    "    def clone(self):\n",
    "        return self.__class__(self.data.clone(), self.sample_rate, self.fn)\n",
    "\n",
    "    def apply_tfms(self, tfms, **kwargs):\n",
    "        for tfm in tfms:\n",
    "            self.data = tfm(self.data, **kwargs)\n",
    "            if issubclass(type(tfm), MyDataAugmentation):\n",
    "                self.processed_signal = self.data.clone().cpu()\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        return self.num_samples / self.sample_rate\n",
    "\n",
    "    def show(self, ax=None, figsize=(5, 1), player=True, title=None, **kwargs):\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots(figsize=figsize)\n",
    "        if title:\n",
    "            ax.set_title(\"Class: \" + str(title) + \" \\nfilename: \" + str(self.fn))\n",
    "        \n",
    "        timesteps = np.arange(self.original_signal.shape[1]) / self.sample_rate\n",
    "        \n",
    "        ax.plot(timesteps, self.original_signal[0]) \n",
    "        if self.original_signal.size(0) > 1: # Check if mono or stereo\n",
    "            ax.plot(timesteps, self.original_signal[1]) \n",
    "        ax.set_xlabel('Original Signal Time (s)')\n",
    "        plt.show()\n",
    "        \n",
    "        timesteps = np.arange(self.processed_signal.shape[1]) / self.sample_rate\n",
    "\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "        if title:\n",
    "            ax.set_title(\"Class: \" + str(title) + \" \\nfilename: \" + str(self.fn))\n",
    "        ax.plot(timesteps, self.processed_signal[0]) \n",
    "        if self.processed_signal.size(0) > 1: # Check if mono or stereo\n",
    "            ax.plot(timesteps, self.processed_signal[1]) \n",
    "        ax.set_xlabel('Processed Signal Time (s)')\n",
    "        plt.show()\n",
    "        \n",
    "        if player:\n",
    "            # unable to display an IPython 'Audio' player in plt axes\n",
    "            display(\"Original signal\")\n",
    "            display(Audio(self.original_signal, rate=self.sample_rate))\n",
    "            display(\"Processed signal\")\n",
    "            display(Audio(self.processed_signal, rate=self.sample_rate))\n",
    "\n",
    "def open_audio(fn, using_librosa:bool=False, downsampling=8000):\n",
    "    if using_librosa: \n",
    "        x, sr = librosa.core.load(fn, sr=None, mono=False)\n",
    "        \n",
    "    else:\n",
    "        if soundfile_spec is not None:\n",
    "            x, sr = sf.read(fn, always_2d=True, dtype=\"float32\")\n",
    "        else:\n",
    "            raise Exception(\"Cannot load soundfile\")\n",
    "            #sr, x = wavfile.read(fn) # 10 times faster than librosa but issues with 24bits wave\n",
    "    \n",
    "    if len(x.shape) == 1: # Mono signal\n",
    "        x = x.reshape(1, -1) # Add 1 channel\n",
    "    else:\n",
    "        if not using_librosa:\n",
    "            x = np.swapaxes(x, 1, 0) # Scipy result is timestep * channels instead of channels * timestep\n",
    "    \n",
    "    if downsampling is not None:\n",
    "        x = librosa.core.resample(x, sr, downsampling)\n",
    "        sr = downsampling\n",
    "    t = torch.from_numpy(x.astype(np.float32, copy=False))\n",
    "    if x.dtype == np.int16:\n",
    "        t.div_(32767)\n",
    "    elif x.dtype != np.float32:\n",
    "        raise OSError('Encountered unexpected dtype: {}'.format(x.dtype))\n",
    "    return AudioClip(t, sr, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataBunch(DataBunch):\n",
    "    \n",
    "    # Subclass because of bug to give dl_tfms to underlying dataloader\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, \n",
    "               tfms:Optional[Collection[Callable]]=None, # There is a bug in LabelLists because dl_tfms is not given to dataloader\n",
    "               **kwargs)->'AudioDataBunch':\n",
    "        db = super().create(train_ds=train_ds, valid_ds=valid_ds, dl_tfms=tfms, **kwargs)\n",
    "\n",
    "        return db\n",
    "\n",
    "\n",
    "    def show_batch(self, rows:int=5, ds_type:DatasetType=DatasetType.Train, **kwargs):\n",
    "        dl = self.dl(ds_type)\n",
    "        ds = dl.dl.dataset\n",
    "\n",
    "        idx = np.random.choice(len(ds), size=rows, replace=False)\n",
    "        batch = ds[idx]\n",
    "        \n",
    "        max_count = min(rows, len(batch))\n",
    "        xs, ys, xs_processed, ys_processed = [], [], [], []\n",
    "        for i in range(max_count):\n",
    "            x, x_processed, y, y_processed = batch[i][0], batch[i][0].data, batch[i][1], torch.tensor(batch[i][1].data)\n",
    "            xs.append(x)\n",
    "            xs_processed.append(x_processed)\n",
    "            ys.append(y)\n",
    "            ys_processed.append(y_processed)\n",
    "\n",
    "        xs_processed = torch.stack(xs_processed, dim=0)\n",
    "        ys_processed = torch.stack(ys_processed, dim=0)\n",
    "        \n",
    "        for tfm in dl.tfms:\n",
    "            xs_processed, ys_processed = tfm((xs_processed, ys_processed))\n",
    "\n",
    "        \n",
    "        self.train_ds.show_xys(xs, ys, xs_processed=xs_processed.unbind(dim=0), **kwargs)\n",
    "        del xs, ys, xs_processed, ys_processed\n",
    "\n",
    "    # Inspired by ImageDataBunch\n",
    "    def batch_stats(self, funcs:Collection[Callable]=None, ds_type:DatasetType=DatasetType.Train)->Tensor:\n",
    "        \"Grab a batch of data and call reduction function `func` per channel\"\n",
    "        funcs = ifnone(funcs, [torch.mean,torch.std])\n",
    "        x = self.one_batch(ds_type=ds_type, denorm=False)[0].cpu()\n",
    "        return [func(channel_view(x), 1) for func in funcs]\n",
    "        \n",
    "    # Inspired by ImageDataBunch\n",
    "    def normalize(self, stats:Collection[Tensor]=None, do_x:bool=True, do_y:bool=False)->None:\n",
    "        \"Add normalize transform using `stats` (defaults to `DataBunch.batch_stats`)\"\n",
    "        if getattr(self,'norm',False): raise Exception('Can not call normalize twice')\n",
    "        if stats is None: self.stats = self.batch_stats()\n",
    "        else:             self.stats = stats\n",
    "        self.norm,self.denorm = normalize_funcs(*self.stats, do_x=do_x, do_y=do_y)\n",
    "        self.add_tfm(self.norm)\n",
    "        return self\n",
    "\n",
    "       \n",
    "\n",
    "# Inspired by https://docs.fast.ai/tutorial.itemlist.html\n",
    "class AudioItemList(ItemList):\n",
    "    _bunch = AudioDataBunch # Needed to include normalize\n",
    "    \n",
    "    def __init__(self, items:Iterator, using_librosa=False, downsampling=None, **kwargs):\n",
    "        super().__init__(items=items, **kwargs)\n",
    "        self.using_librosa = using_librosa\n",
    "        self.copy_new.append('using_librosa')\n",
    "        self.downsampling = downsampling\n",
    "        self.copy_new.append('downsampling')\n",
    "\n",
    "    def get(self, i):\n",
    "        fn = super().get(i)\n",
    "        \n",
    "        return open_audio(fn, using_librosa=self.using_librosa, downsampling=self.downsampling)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_df(cls, df, path, using_librosa=False, folder:PathOrStr=None, downsampling=None, **kwargs):\n",
    "        #if folder is not None: path2 = str(path)+folder\n",
    "        res = super().from_df(df, path=path, **kwargs)\n",
    "        pref = f'{res.path}{os.path.sep}'\n",
    "        if folder is not None: pref += f'{folder}{os.path.sep}'\n",
    "        res.items = np.char.add(pref, res.items.astype(str))\n",
    "        res.using_librosa=using_librosa\n",
    "        res.downsampling = downsampling\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def reconstruct(self, t:Tensor, x:Tensor = None): \n",
    "        raise Exception(\"Not implemented yet\")\n",
    "        # From torch\n",
    "        #return ImagePoints(FlowField(x.size, t), scale=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    def show_xys(self, xs, ys, xs_processed=None, figsize=None, **kwargs):\n",
    "        if xs_processed is None:\n",
    "            for x, y in zip(xs, ys):\n",
    "                x.show(title=str(y), figsize=figsize, **kwargs)\n",
    "        else:\n",
    "            for x, y, x_processed in zip(xs, ys, xs_processed):\n",
    "                x.show(title=str(y), figsize=figsize, **kwargs)\n",
    "                for channel in range(x_processed.size(0)):\n",
    "                    Image(x_processed[channel, :, :].unsqueeze(0)).show(figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from fastai.vision.learner, omitting unused args,\n",
    "# and adding channel summing of first convolutional layer\n",
    "def create_cnn(data, arch, pretrained=False, is_mono_input=True, **kwargs):\n",
    "    meta = cnn_config(arch)\n",
    "    body = create_body(arch, pretrained)\n",
    "\n",
    "    # sum up the weights of in_channels axis, to reduce to single input channel\n",
    "    # Suggestion by David Gutman\n",
    "    # https://forums.fast.ai/t/black-and-white-images-on-vgg16/2479/2\n",
    "    if is_mono_input:\n",
    "        first_conv_layer = body[0][0]\n",
    "        first_conv_weights = first_conv_layer.state_dict()['weight']\n",
    "        assert first_conv_weights.size(1) == 3 # RGB channels dim\n",
    "        summed_weights = torch.sum(first_conv_weights, dim=1, keepdim=True)\n",
    "        first_conv_layer.weight.data = summed_weights\n",
    "        first_conv_layer.in_channels = 1\n",
    "    else:\n",
    "        # In this case, the input is a stereo\n",
    "        first_conv_layer = body[0]\n",
    "        first_conv_weights = first_conv_layer.state_dict()['weight']\n",
    "        assert first_conv_weights.size(1) == 3 # RGB channels dim\n",
    "        summed_weights = torch.sum(first_conv_weights, dim=1, keepdim=True)\n",
    "        first_conv_layer.weight.data = first_conv_weights[:, :2, :, :] # Keep only 2 channels for the weights\n",
    "        first_conv_layer.in_channels = 2\n",
    "\n",
    "    nf = num_features_model(body) * 2\n",
    "    head = create_head(nf, data.c, None, 0.5)\n",
    "    model = nn.Sequential(body, head)\n",
    "    learn = Learner(data, model, **kwargs)\n",
    "    learn.split(meta['split'])\n",
    "    if pretrained:\n",
    "        learn.freeze()\n",
    "    apply_init(model[1], nn.init.kaiming_normal_)\n",
    "    return learn\n",
    "\n",
    "\n",
    "\n",
    "def my_cl_int_plot_top_losses(self, k, largest=True, figsize=(25,7), heatmap:bool=True, heatmap_thresh:int=16,\n",
    "                            return_fig:bool=None)->Optional[plt.Figure]:\n",
    "    \"Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.\"\n",
    "    tl_val,tl_idx = self.top_losses(k, largest)\n",
    "    classes = self.data.classes\n",
    "    cols = math.ceil(math.sqrt(k))\n",
    "    rows = math.ceil(k/cols)\n",
    "    fig,axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    fig.suptitle('prediction/actual/loss/probability', weight='bold', size=14)\n",
    "    for i,idx in enumerate(tl_idx):\n",
    "        audio, cl = self.data.dl(self.ds_type).dataset[idx]\n",
    "        audio = audio.clone()\n",
    "        \n",
    "        m = self.learn.model.eval()\n",
    "        \n",
    "        x, _ = self.data.one_item(audio) # Process one audio into prediction\n",
    "        \n",
    "        x_consolidated = x.sum(dim=1, keepdim=True) # Sum accross all channels to ease the interpretation\n",
    "\n",
    "        im = Image(x_consolidated[0, :, :, :].cpu()) # Extract the processed image from the prediction (after dl_tfms) and keep it into CPU\n",
    "        cl = int(cl)\n",
    "        title = f'{classes[self.pred_class[idx]]}/{classes[cl]} / {self.losses[idx]:.2f} / {self.probs[idx][cl]:.2f}'\n",
    "        title = title + f'\\n {audio.fn}'\n",
    "        \n",
    "        im.show(ax=axes.flat[i], title=title)\n",
    "        \n",
    "        if heatmap:\n",
    "            # Related paper http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf\n",
    "            with hook_output(m[0]) as hook_a: # hook activations from CNN module\n",
    "                with hook_output(m[0], grad= True) as hook_g: # hook gradients from CNN module\n",
    "                    preds = m(x) # Forward pass to get activations\n",
    "                    preds[0,cl].backward() # Backward pass to get gradients\n",
    "            acts = hook_a.stored[0].cpu()\n",
    "            if (acts.shape[-1]*acts.shape[-2]) >= heatmap_thresh:\n",
    "                grad = hook_g.stored[0][0].cpu() # Hook the gradients from the CNN module and extract the first one (because one item only)\n",
    "                grad_chan = grad.mean(1).mean(1) # Mean accross image to keep mean gradients per channel \n",
    "                mult = F.relu(((acts*grad_chan[...,None,None])).sum(0)) # Multiply activation with gradients (add 1 dim for height and width)\n",
    "                sz = list(im.shape[-2:])\n",
    "                axes.flat[i].imshow(mult, alpha=0.35, extent=(0,*sz[::-1],0), interpolation='bilinear', cmap='magma')     \n",
    "        \n",
    "    if ifnone(return_fig, defaults.return_fig): return fig\n",
    "    \n",
    "    \n",
    "ClassificationInterpretation.plot_audio_top_losses = my_cl_int_plot_top_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk_np(preds, targs, k=3):\n",
    "    preds = np.argsort(-preds, axis=1)[:, :k]\n",
    "    score = 0.\n",
    "    for i in range(k):\n",
    "        num_hits = (preds[:, i] == targs).sum()\n",
    "        score += num_hits * (1. / (i+1.))\n",
    "    score /= preds.shape[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def mapk(preds, targs, k=3):\n",
    "    return tensor(mapk_np(to_np(preds), to_np(targs), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_augmentation_transforms(max_seconds=30, start_at_second=0,\n",
    "                                     sample_rate=44100, noise_scl=None, convert_to_mono=True):\n",
    "    tfms = []\n",
    "    if convert_to_mono:\n",
    "        tfms.append(ConvertToMono())\n",
    "    max_channels = 1 if convert_to_mono else 2\n",
    "    tfms.append(PadToMax(start_at_second=start_at_second, max_seconds=max_seconds, \n",
    "                         sample_rate=sample_rate, max_channels=max_channels))\n",
    "    \n",
    "    if noise_scl is not None:\n",
    "        tfms.append(WhiteNoise(noise_scl))\n",
    "    return tfms\n",
    "\n",
    "def get_frequency_transforms(n_fft=512, n_hop=160, top_db=80,\n",
    "                             n_mels=None, f_min=0, f_max=None, sample_rate=44100):\n",
    "#    tfms.append(MFCC(n_fft=n_fft, n_mfcc=n_mels, hop_length=n_hop, sample_rate=sample_rate, f_min=f_min, f_max=f_max))\n",
    "    tfms = [Spectrogram(n_fft=n_fft, n_hop=n_hop)]\n",
    "    tfms.append(FrequencyToMel(n_fft=n_fft, n_mels=n_mels, sr=sample_rate, f_min=f_min, f_max=f_max))\n",
    "    tfms.append(ToDecibels(top_db=top_db))\n",
    "    \n",
    "    return tfms\n",
    "\n",
    "\n",
    "def get_frequency_batch_transforms(*args, **kwargs):\n",
    "    tfms = get_frequency_transforms(*args, **kwargs)\n",
    "\n",
    "    def _freq_batch_transformer(inputs):\n",
    "        xs, ys = inputs\n",
    "        for tfm in tfms:\n",
    "            xs = tfm(xs)\n",
    "        del inputs\n",
    "        \n",
    "        return xs, ys.detach()\n",
    "    return [_freq_batch_transformer]\n",
    "\n",
    "# Parent classes used to distinguish transforms for data augmentation and transforms to convert audio into image\n",
    "class MyDataAugmentation:\n",
    "    pass\n",
    "\n",
    "class MySoundToImage:\n",
    "    pass\n",
    "\n",
    "### The below transformers are on the single AudioClip (to help to keep tracks of changes from data augmentation)\n",
    "\n",
    "class ConvertToMono(MyDataAugmentation):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, X):\n",
    "        assert(X.dim() == 2) # channels * timestep\n",
    "        X = X.sum(0) # Sum over channels\n",
    "        X = X.unsqueeze(0)\n",
    "        assert(X.dim() == 2) # channels * timestep\n",
    "        return X\n",
    "\n",
    "    \n",
    "    \n",
    "class PadToMax(MyDataAugmentation):\n",
    "    def __init__(self, start_at_second=0, max_seconds=30, sample_rate=16000, max_channels=1):\n",
    "        self.max_seconds = max_seconds\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_channels = max_channels\n",
    "        self.start_at_second = start_at_second\n",
    "        \n",
    "\n",
    "    def __call__(self, X):\n",
    "        # X must be channels * timestep\n",
    "        assert(X.dim() == 2)\n",
    "        assert(X.size(0) <= 2) # There is only 2 channels at maximum \n",
    "        \n",
    "        mx = int(self.max_seconds * self.sample_rate)\n",
    "        start_at = min(int(self.start_at_second * self.sample_rate), X.size(1))\n",
    "        if X.size(1) - start_at <= mx:\n",
    "            start_at = max(X.size(1) - mx, 0)\n",
    "        \n",
    "        if (X.size(1) < mx): \n",
    "            X = torch.cat((X, torch.zeros([X.size(0), mx - X.size(1)], device=X.device)), dim=1) # Channels * Timestep\n",
    "        if (X.size(1) > mx): \n",
    "            X = X[:, start_at:(mx + start_at)]\n",
    "        if X.size(0) < self.max_channels:\n",
    "            targets = torch.zeros(self.max_channels, X.size(1), device=X.device)\n",
    "            targets[:X.size(0), :] = X\n",
    "            X = targets\n",
    "        \n",
    "        return X\n",
    "\n",
    "    \n",
    "class WhiteNoise(MyDataAugmentation):\n",
    "    def __init__(self, noise_scl=0.0005):\n",
    "        self.noise_scl= noise_scl\n",
    "\n",
    "    def __call__(self, X):\n",
    "        noise = torch.randn(X.shape, device=X.device) * self.noise_scl \n",
    "        assert(X.dim() == 2) # channels * timestep\n",
    "        return X + noise\n",
    "\n",
    "    \n",
    "### The below transformers are on the whole batch\n",
    "\n",
    "    \n",
    "class MFCCLibrosa(MySoundToImage):\n",
    "    def __init__(self, sample_rate=16000, n_mfcc=20, n_fft=512, hop_length=512, f_min=0, f_max=None):\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.f_min=f_min\n",
    "        self.f_max=f_max\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        mfcc = torch.zeros([X.size(0), self.n_mfcc, 1+int(X.size(1) / self.hop_length)], device=X.device)\n",
    "        for i in range(X.size(0)):\n",
    "            single_mfcc = librosa.feature.mfcc(y=X[0, :].cpu().numpy(), \n",
    "                                   sr=self.sample_rate, n_mfcc=self.n_mfcc, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                                         fmin=self.f_min, fmax=self.f_max)\n",
    "            mfcc[i, :, :] = torch.tensor(single_mfcc, device=X.device)\n",
    "        del X\n",
    "        return mfcc\n",
    "    \n",
    "# Returns power spectrogram (magnitude squared)\n",
    "class Spectrogram(MySoundToImage):\n",
    "    def __init__(self, n_fft=1024, n_hop=256, window=torch.hann_window,\n",
    "                 device=None):\n",
    "        self.n_fft = n_fft\n",
    "        self.n_hop = n_hop\n",
    "        self.window = window(n_fft)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        X_left = torch.stft(x[:, 0, :],\n",
    "                       n_fft=self.n_fft,\n",
    "                       hop_length=self.n_hop,\n",
    "                       win_length=self.n_fft,\n",
    "                       window=to_device(self.window, x.device),\n",
    "                       onesided=True,\n",
    "                       center=True,\n",
    "                       pad_mode='constant',\n",
    "                       normalized=True)\n",
    "        # compute power from real and imag parts (magnitude^2)\n",
    "        X_left.pow_(2.0)\n",
    "        X_left = X_left[:,:,:,0] + X_left[:,:,:,1]\n",
    "        X_left = X_left.unsqueeze(1) # Add channel dimension\n",
    "\n",
    "        if (x.size(1) > 1):\n",
    "            X_right = torch.stft(x[:, 1, :],\n",
    "                           n_fft=self.n_fft,\n",
    "                           hop_length=self.n_hop,\n",
    "                           win_length=self.n_fft,\n",
    "                           window=to_device(self.window, x.device),\n",
    "                           onesided=True,\n",
    "                           center=True,\n",
    "                           pad_mode='constant',\n",
    "                           normalized=True)        \n",
    "            # compute power from real and imag parts (magnitude^2)\n",
    "            X_right.pow_(2.0)\n",
    "            X_right = X_right[:,:,:,0] + X_right[:,:,:,1]\n",
    "            X_right = X_right.unsqueeze(1) # Add channel dimension\n",
    "            res = torch.cat([X_left, X_right], dim=1) \n",
    "            assert(res.dim() == 4) # Check dim (n sample * channels * h * w)\n",
    "            return res\n",
    "            \n",
    "        else:\n",
    "            assert(X_left.dim() == 4) # Check dim (n sample * channels * h * w)\n",
    "            return X_left # Return only mono channel\n",
    "        \n",
    "    \n",
    "class FrequencyToMel(MySoundToImage):\n",
    "    def __init__(self, n_mels=40, n_fft=1024, sr=16000,\n",
    "                 f_min=0.0, f_max=None, device=None):\n",
    "        self.mel_fb = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "                                fmin=f_min, fmax=f_max).astype(np.float32)\n",
    "\n",
    "    def __call__(self, spec_f):\n",
    "        spec_m = to_device(torch.from_numpy(self.mel_fb), spec_f.device) @ spec_f\n",
    "        assert(spec_m.dim() == 4) # Check dim (n sample * channels * h * w)\n",
    "        return spec_m\n",
    "\n",
    "\n",
    "class ToDecibels(MySoundToImage):\n",
    "    def __init__(self,\n",
    "                 power=2, # magnitude=1, power=2\n",
    "                 ref=1.0,\n",
    "                 top_db=None,\n",
    "                 normalized=True,\n",
    "                 amin=1e-7):\n",
    "        self.constant = 10.0 if power == 2 else 20.0\n",
    "        self.ref = ref\n",
    "        self.top_db = abs(top_db) if top_db else top_db\n",
    "        self.normalized = normalized\n",
    "        self.amin = amin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        if self.ref == 'max':\n",
    "            ref_value = x.contiguous().view(batch_size, -1).max(dim=-1)[0]\n",
    "            ref_value.unsqueeze_(1).unsqueeze_(1)\n",
    "        else:\n",
    "            ref_value = tensor(self.ref)\n",
    "        spec_db = x.clamp_min(self.amin).log10_().mul_(self.constant)\n",
    "        spec_db.sub_(ref_value.clamp_min_(self.amin).log10_().mul_(10.0))\n",
    "        if self.top_db is not None:\n",
    "            max_spec = spec_db.view(batch_size, -1).max(dim=-1)[0]\n",
    "            max_spec.unsqueeze_(1).unsqueeze_(1).unsqueeze_(1)\n",
    "            spec_db = torch.max(spec_db, max_spec - self.top_db)\n",
    "            if self.normalized:\n",
    "                # normalize to [0, 1]\n",
    "                spec_db.add_(self.top_db).div_(self.top_db)\n",
    "        assert(spec_db.dim() == 4) # Check dim (n sample * channels * h * w)\n",
    "        return spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Brings TTA (Test Time Functionality) to the `Learner` class. Use `learner.TTA()` instead\"\n",
    "from fastai.torch_core import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_train import _loss_func2activ\n",
    "from fastai.basic_data import DatasetType\n",
    "\n",
    "__all__ = []\n",
    "\n",
    "\n",
    "def _tta_only(learn:Learner, ds_type:DatasetType=DatasetType.Valid) -> Iterator[List[Tensor]]:\n",
    "    \"Computes the outputs for several augmented inputs for TTA\"\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "    old = ds.tfms\n",
    "    augm_tfm = [o for o in learn.data.train_ds.tfms]\n",
    "    try:\n",
    "        pbar = master_bar(range(8))\n",
    "        for i in pbar:\n",
    "            ds.tfms = augm_tfm\n",
    "            yield get_preds(learn.model, dl, pbar=pbar, activ=_loss_func2activ(learn.loss_func))[0]\n",
    "    finally: ds.tfms = old\n",
    "\n",
    "\n",
    "Learner.tta_only = _tta_only\n",
    "\n",
    "\n",
    "def _TTA(learn:Learner, beta:float=0.4, ds_type:DatasetType=DatasetType.Valid, with_loss:bool=False) -> Tensors:\n",
    "    \"Applies TTA to predict on `ds_type` dataset.\"\n",
    "    preds,y = learn.get_preds(ds_type)\n",
    "    all_preds = list(learn.tta_only(ds_type=ds_type))\n",
    "    avg_preds = torch.stack(all_preds).mean(0)\n",
    "    if beta is None: return preds,avg_preds,y\n",
    "    else:\n",
    "        final_preds = preds*beta + avg_preds*(1-beta)\n",
    "        if with_loss:\n",
    "            return final_preds, y, calc_loss(final_preds, y, learn.loss_func)\n",
    "        return final_preds, y\n",
    "\n",
    "\n",
    "Learner.TTA = _TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('data')\n",
    "CSV_TRN_CURATED = DATA/'train_curated.csv'\n",
    "CSV_TRN_NOISY = DATA/'train_noisy.csv'\n",
    "CSV_SUBMISSION = DATA/'sample_submission.csv'\n",
    "TRN_CURATED = DATA/'train_curated'\n",
    "TRN_NOISY = DATA/'train_noisy'\n",
    "TEST = DATA/'test'\n",
    "\n",
    "WORK = Path('work')\n",
    "IMG_TRN_CURATED = WORK/'image/trn_curated'\n",
    "IMG_TRN_NOISY = WORK/'image/trn_curated'\n",
    "IMG_TEST = WORK/'image/test'\n",
    "for folder in [WORK, IMG_TRN_CURATED, IMG_TRN_NOISY, IMG_TEST]: \n",
    "    Path(folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "df = pd.read_csv(CSV_TRN_CURATED)\n",
    "df_n = pd.read_csv(CSV_TRN_NOISY)\n",
    "test_df = pd.read_csv(CSV_SUBMISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/.local/lib/python3.6/site-packages/librosa/filters.py:284: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    }
   ],
   "source": [
    "n_fft = 512 # output of fft will have shape [513 x n_frames]\n",
    "n_hop = 94  # width of Spectogram = max_seconds * sample rate / n_hop\n",
    "n_mels = 128 # Height of spectogram\n",
    "sample_rate = 48127\n",
    "max_seconds = 2\n",
    "f_min=0\n",
    "f_max=8000\n",
    "noise_scl=0.005\n",
    "\n",
    "\n",
    "train_tfms = get_data_augmentation_transforms(sample_rate=sample_rate, max_seconds=max_seconds, \n",
    "                                              noise_scl=noise_scl)\n",
    "valid_tfms = get_data_augmentation_transforms(sample_rate=sample_rate, max_seconds=max_seconds)\n",
    "\n",
    "dl_tfms = get_frequency_batch_transforms(n_fft=n_fft, n_hop=n_hop,\n",
    "                                            n_mels=n_mels, \n",
    "                                            f_min=f_min, f_max=f_max,\n",
    "                                            sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "audios = (AudioItemList.from_df(df=df, path='data', folder='train_merged', using_librosa=True)\n",
    "          .split_by_rand_pct(0.1)\n",
    "          .label_from_df(label_delim=',')\n",
    "          .add_test_folder('test')\n",
    "          .transform(tfms=(train_tfms, valid_tfms))\n",
    "          .databunch(bs=batch_size, tfms=dl_tfms)\n",
    "         ).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4473, 497, 1120)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios.c, len(audios.train_ds), len(audios.valid_ds), len(audios.test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 1024]) torch.Size([32, 80])\n"
     ]
    }
   ],
   "source": [
    "xs, ys = audios.one_batch()\n",
    "print(xs.shape, ys.shape)\n",
    "del xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios.show_batch(3, ds_type=DatasetType.Train, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score = partial(fbeta, thresh=0.2)\n",
    "acc_02 = partial(accuracy_thresh, thresh=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = create_cnn(audios, models.vgg16_bn, pretrained=False, metrics=[f_score, acc_02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 11.91 GiB total capacity; 9.92 GiB already allocated; 187.19 MiB free; 225.82 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c7a9c29f9dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fastai/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m~/git/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 180.00 MiB (GPU 0; 11.91 GiB total capacity; 9.92 GiB already allocated; 187.19 MiB free; 225.82 MiB cached)"
     ]
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
