{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most obvious problems with our model is that it operates on fixed lengths of audio clips while our dataset contains audio clips of various lengths. We would like to improve our model's performance on long clips by re-running it on different portions of the clip and combining the predictions, though it's not obvious how exactly we should combine them. \n",
    "\n",
    "We're taking inspiration from: https://github.com/fastai/fastai/blob/master/fastai/vision/tta.py#L10-L45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import PIL\n",
    "import fastai\n",
    "from fastai.basic_train import _loss_func2activ\n",
    "from fastai.vision.data import pil2tensor\n",
    "from fastai.vision import Path, get_preds, ImageList, Image, imagenet_stats, Learner, cnn_learner, get_transforms, DatasetType, models, load_learner, fbeta\n",
    "import sklearn.metrics\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff275eef170>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 01_BasicModel__folds5\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "script_name = os.path.basename('01_BasicModel').split('.')[0]\n",
    "MODEL_NAME = \"{0}__folds{1}\".format(script_name, NFOLDS)\n",
    "print(\"Model: {}\".format(MODEL_NAME))\n",
    "\n",
    "# Make required folders if they're not already present\n",
    "directories = ['kfolds', 'model_predictions', 'model_source']\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('data')\n",
    "WORK = Path('work')\n",
    "\n",
    "CSV_TRN_MERGED = DATA/'train_merged.csv'\n",
    "CSV_SUBMISSION = DATA/'sample_submission.csv'\n",
    "\n",
    "TRN_CURATED = DATA/'train_curated2'\n",
    "TRN_NOISY = DATA/'train_noisy2'\n",
    "\n",
    "IMG_TRN_CURATED = WORK/'image/trn_curated2'\n",
    "IMG_TRN_NOISY = WORK/'image/trn_noisy2'\n",
    "IMG_TEST = WORK/'image/test'\n",
    "\n",
    "TEST = DATA/'test'\n",
    "\n",
    "train = pd.read_csv(DATA/'train_curated.csv')\n",
    "test = pd.read_csv(DATA/'sample_submission.csv')\n",
    "train_noisy = pd.read_csv(DATA/'train_noisy.csv')\n",
    "train_merged = pd.read_csv(DATA/'train_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['fname']\n",
    "y = train['labels'].apply(lambda f: f.split(','))\n",
    "y_noisy = train_noisy['labels'].apply(lambda f: f.split(','))\n",
    "transformed_y = MultiLabelBinarizer().fit_transform(y)\n",
    "transformed_y_noisy = MultiLabelBinarizer().fit_transform(y_noisy)\n",
    "filenames = train['fname'].values\n",
    "filenames = filenames.reshape(-1, 1)\n",
    "\n",
    "oof_preds = np.zeros((len(train), 80))\n",
    "test_preds = np.zeros((len(test), 80))\n",
    "\n",
    "tfms = get_transforms(do_flip=True, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
    "_, val_index = next(mskf.split(X, transformed_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our clasifier stuff    \n",
    "src = (ImageList.from_csv(WORK/'image', Path('../../')/DATA/'train_curated.csv', folder='trn_merged', suffix='.jpg')\n",
    "    .split_by_idx(val_index)\n",
    "    #.label_from_df(cols=list(train_merged.columns[1:]))\n",
    "    .label_from_df(label_delim=',')\n",
    "      )\n",
    "\n",
    "data = (src.transform(tfms, size=128).databunch(bs=64).normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fbeta</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.637949</td>\n",
       "      <td>0.246274</td>\n",
       "      <td>0.080768</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.212277</td>\n",
       "      <td>0.068762</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109087</td>\n",
       "      <td>0.071224</td>\n",
       "      <td>0.060612</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079570</td>\n",
       "      <td>0.061587</td>\n",
       "      <td>0.086942</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068955</td>\n",
       "      <td>0.056947</td>\n",
       "      <td>0.153332</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.063663</td>\n",
       "      <td>0.052556</td>\n",
       "      <td>0.168737</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.275146</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056710</td>\n",
       "      <td>0.043019</td>\n",
       "      <td>0.352545</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.053919</td>\n",
       "      <td>0.040337</td>\n",
       "      <td>0.384566</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.051809</td>\n",
       "      <td>0.039240</td>\n",
       "      <td>0.422933</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_score = partial(fbeta, thresh=0.2)\n",
    "learn = cnn_learner(data, models.xresnet101, pretrained=False, metrics=[f_score]).mixup(stack_y=False)\n",
    "learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overrides fastai's default 'open_image' method to crop based on our crop counter\n",
    "def setupNewCrop(counter):\n",
    "    \n",
    "    def open_fat2019_image(fn, convert_mode, after_open)->Image:\n",
    "        \n",
    "        x = PIL.Image.open(fn).convert('L')\n",
    "\n",
    "        # crop (128x321 for a 5 second long audio clip)\n",
    "        time_dim, base_dim = x.size\n",
    "\n",
    "        #How many crops can we take?\n",
    "        maxCrops = int(np.ceil(base_dim / time_dim))\n",
    "        \n",
    "        #What's the furthest point at which we can take a crop without running out of pixels\n",
    "        lastValidCrop = time_dim - base_dim\n",
    "\n",
    "        crop_x = (counter % maxCrops) * time_dim \n",
    "\n",
    "        # We don't want to crop any further than the last 128 pixels\n",
    "        crop_x = min(crop_x, lastValidCrop)\n",
    "\n",
    "        x1 = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n",
    "        \n",
    "        newImage = np.stack([x1,x1,x1], axis=-1)\n",
    "\n",
    "        # standardize    \n",
    "        return Image(pil2tensor(newImage, np.float32).div_(255))\n",
    "\n",
    "    fastai.vision.data.open_image = open_fat2019_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tta(learn:Learner, ds_type:DatasetType=DatasetType.Valid):\n",
    "    dl = learn.dl(ds_type)\n",
    "    ds = dl.dataset\n",
    "\n",
    "    old_open_image = fastai.vision.data.open_image\n",
    "    try:\n",
    "        maxNumberOfCrops = 25\n",
    "        for i in range(maxNumberOfCrops):\n",
    "            print(\"starting\")\n",
    "            setupNewCrop(i)\n",
    "            yield get_preds(learn.model, dl, activ=_loss_func2activ(learn.loss_func))[0]\n",
    "    finally:\n",
    "            fastai.vision.data.open_image = old_open_image\n",
    "        \n",
    "all_preds = list(custom_tta(learn))\n",
    "avg_preds = torch.stack(all_preds).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 80])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
